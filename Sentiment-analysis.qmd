---
title: "Sentiment Analysis ANOVAs"
author: 
- "Fynn Hayton-Ruffner"
- "Shekinah Ninziza"
- "Kinglee Tham"
---


```{r}
#| label: setup
#| include: false

# Improve digit display and NA display for kable tables
options(scipen = 1, knitr.kable.NA = "")

# Load necessary packages
library(tidyverse)
library(knitr)
library(kableExtra)

# for text analysis
library(tidytext)
library(tidyverse)
```

# What's engagement and sentiment like across subreddits?


```{r}
#| fig-align: center
#| out-width: 30%
#| out-height: 50px
#| layout: [[1,1,1]]
 
 include_graphics("https://i.imgur.com/WydIOUS.png") # Amherst icon
  include_graphics("https://i.imgur.com/0Au7pMT.png") # Middlebury icon
 include_graphics("https://i.imgur.com/FA3VX3Y.png") # Williams icon


```
 

```{r}
#| label: tbl-summary
#| tbl-cap: "Subreddit sentiment & engagement"

load('./data/subreddit_summaries.Rdata')

subreddit_summaries |>
  kable(digits=2, 
        col.names = c('Subreddit', 'Post Count','Score (tot)', 
                      'Comments (tot)', 'Score (avg)', 'Comments (avg)')) |>
  row_spec(1, background = "#b7a5d3", color = 'white') |>
  row_spec(2, background = "#37538C", color='white') |>
  row_spec(3, background = "#FFBE0A", color = 'white')

```

Let's compare the subreddits! @tbl-summary above shows the summary statistics for each subreddit. The values were found by grouping our scraped, sentiment-annotated datasetby subreddit and calculating sums and averages of post count, comment count, and sentiment score. If you want to explore the data further to see how this table came to be check out the full dataset [here](index.qmd#fig-raw-data). The total post counts were comparable, which is what we expected given their similar ages. For sentiment, Middlebury clearly seems stands out as more negative (2.95/post) compared to Amherst (5.26) and Williams (4.81). Amherst and Williams also tend to have higher engagement: 4.40 and 6.28 respectively relative to Midd (2.36). These numbers are fun to compare, but, to assess the statistical significance of these potential differences, you have to go a few steps further.

To make any conclusions about whether significant differences exist between the average sentiment scores and comment counts of posts on these subreddits, we needed to use an ANOVA test...or did we?? Unfortunately, ANOVA relies on a few pesky assumptions that must hold true for reliable interpretation. If you trust us, feel free to skip down to the results, but we encourage any curious or statistically inclined reader to hold us accountable and step through our process of checking the assumptions below.

## ANOVA (Analysis of Variance Tests)

### Independence of Observations

This assumption would only violated if the sentiment scores and comment counts of posts were somehow influenced by each other. Since this was not the case, we were able to move past this assumption without too much stress. 

### Normality

The next assumption for ANOVA is normality, which in our case requires the sampling distribution of the mean sentiment score and comment count for each subreddit to follow a normal distribution. We'll use the example of one subreddit's sentiment score to make this more clear, but note that the following applies to all subreddits and both means of interest. If you were to take a large number of post samples, each of size N from the Amherst subreddit, calculate the mean sentiment score and comment count for each sample, and plot those means on a graph, the sampling distribution of the mean sentiment score of the Amherst subreddit would be normal if it looked something like this:

```{r}
#| fig-align: center
#| out-width: 100%

 
include_graphics("https://www.msicertified.com/wp-content/uploads/2023/01/Standard_deviation_diagram.png")


```

ANOVA tries to assess whether or not there are any significant differences between the means of three or more groups, and it requires the sampling distribution of each mean for each group resemble that bell curve above. So how can we tell if this sampling distribution is normal if we just have one sample of posts per subreddit? Quite fortunately, in our case, we were able to rely on the Central Limit Theorem. This theorem states that as long as the sample size is greater than 30, the sampling distribution of the mean of a group will be normal CITE HERE. In our case, our sample sizes for each subreddit far exceeded thirty, so we could safely assume the normality assumption is satisfied here.

### Equal Variances

With normality accounted for, we approached the question of equal variances across subreddit. Variance is simply the average squared difference from the mean of all points in a dataset. There are multiple visual and computational methods to compare the variances of each group, but we decided to use Levine's Test CITE HERE, a metric that only relies on independent observations,
a continuous dependent variable (we have sentiment score and comment count), and a categorical grouping variable (we have subreddit). We already satisfied independence, so it was valid to perform this test of equal variance. The results of Levine's test can is available in REFERENCE. Unfortunately, the tests indicated that both sentiment score (p < 0.001) and comment count (p < 0.001) had different variances across the subreddit groupings, meaning the equal variance assumption would be violated if we conducted a standard ANOVA test for either variable. Foiled right at the end!

## The Solution: Welch's ANOVA

Welch's ANOVA is a variation of the standard ANOVA that can be conducted when the equal variance assumption is violated, which made it perfect for our use!

We fit two one-way Welch-ANOVA models to our dataset, one to test for statiscially significant differences between the average sentiment score of each subreddit, and one to compare the average comment counts per post. The results, available in @tbl-welch-anova-results, were conclusive. indicated there was a least one pairwise difference between subreddit post sentiment average (p < 0.001) and comment count (p < 0.001). The logical follow up was to conduct a post-hoc Tukey's Honestly Significant Difference (HSD), which assess differences between each pair of subreddit means. 

```{r}
#| label: tbl-welch-anova-results
#| tbl-cap: "Welch-ANOVA results: Cross Subreddit Comparisons"
#| tbl-subcap: 
#|    - "Average sentiment per post by subreddit"
#|    - "Average comment count per post by subreddit"
#| layout: [[1, -1, 1]] 
load('./data/welch_results.Rdata')


welch_com |>
  select(-method) |>
  kable(digits=2,
        col.names = c('Df', 'Den Df', 'Statistic', 'p'))
  
welch_sent |>
  select(-method) |>
  kable(digits=2,
        col.names = c('Df', 'Den Df', 'Statistic', 'p'))
# 
# tukey_sent |>
#   select(-term, -null.value) |>
#   kable(digits=2, 
#         col.names = c('Subreddit Pair', 'est', 'low','high', 'p'),
#         caption="Tukey's HSD: subreddit sentiment") |>
#   row_spec(1, background = "#b7a5d3", color = 'white') |>
#   row_spec(3, background = "#FFBE0A", color = 'white')
# 
# 
# 
# tukey_comments |> 
#   select(-term, -null.value) |>
#   kable(col.names = c('Subreddit Pair', 'est', 'low','high', 'p'), 
#         caption = "Tukey's HSD: subreddit comment count",
#         digits=2) |>
#    row_spec(1, background = "#b7a5d3", color = 'white') |>
#    row_spec(2, background = "#FFBE0A", color = 'white') |>
#    row_spec(3, background = "#FFBE0A", color = 'white')
#   
#   

```

The results, see TABLE, were conclusive! Each row in the table above holds the results of a Tukey's HSD comparison between two college subreddits; we colored each row by the college that, according to the test, was deemed to have a significantly higher average (purple for Amherst, gold for Williams, and white for a 'tie'). Both Amherst (p < 0.001) and Williams (p < 0.001) held a significantly higher average sentiment score per post over Middlebury. Notably, Amherst and Williams did not differ signifcantly by this same metric (p = 0.56). As far as engagement goes, Amherst (p < 0.001) and Williams (p < 0.001) yet again out-performed Middlebury in average comments per post, while Williams convincingly exceeded the mean of Amherst as well (p < 0.001). Again, for more information on the gritty details involved in performing Tukey's HSD, see this valuable resource, which was helpful for the work present here: @Tukey. Unfortunately gold holds the majority of victories in these pairwise comparisons, and Williams emerges as the champion of this competition by tying Amherst in post positivity and edging us in post engagement. We wish we could fudge the numbers but that would be statistical malpractice.