---
title: "STAT 231 Lab 8: Text as data"
author: "Fynn Hayton-Ruffner"
date: "2025-03-04"
format: pdf
---

```{r}
#| label: setup
#| include: false

# set code chunk option defaults
knitr::opts_chunk$set(eval=FALSE,
  # display code as types
  tidy = FALSE, 
  # slightly smaller code font
  size = "small",
  # set default figure width and height
  fig.width = 5, fig.height = 3) 

# improve digit and NA display 
options(scipen = 1, knitr.kable.NA = '')

# load packages
library(tidytext)
library(tidyverse)
library(wordcloud)
library(RColorBrewer)
library(ggthemes)
library(textdata)

```


# Getting set up

1. Before editing this file, verify you are working on the file saved in **your private repo** (and NOT the course-content repo). 

2. Change your name in the YAML header above.

3. Save (`Cmd + S` on Mac or `Ctrl + S` on PC), view the changes in this qmd in GitHub Desktop, and commit the initial, incomplete version of the lab (commit message: "Add Lab 8").

4. After committing, go ahead and **Push** your commit back onto GitHub. 

5. Check that your changes updated correctly in your private repo on GitHub.com.

# About this lab

*"Hope" is the thing with feathers --*  
*That perches in the soul --*  
*And sings the tune without the words --*  
*And never stops -- at all --*

*And sweetest -- in the Gale -- is heard --*  
*And sore must be the storm --*  
*That could abash the little Bird*  
*That kept so many warm --*

*I've heard it in the chillest land --*  
*And on the strangest Sea --*  
*Yet -- never -- in Extremity,*  
*It asked a crumb -- of me.*  
- Emily Dickinson (Ref: [Hope is the thing with feathers--](https://en.wikipedia.org/wiki/%22Hope%22_is_the_thing_with_feathers))

Today we're going to analyze Emily Dickinson's poetry.

## Packages

In addition to the familiar **tidyverse** package, there are three new packages we'll be using for this lab:

1. **tidytext**: makes text analysis easier and is consistent with the tools we've been using in the **tidyverse** package
2. **wordcloud**: allows us to visually represent the text data in word clouds
3. **textdata**: allows us to access lexicons for sentiment analysis  

Make sure you load each package in the `setup` code chunk above. 

## The data

The `DickinsonPoems` dataset contains the title and text of the Emily Dickinson poems and can be loaded from the Rds file available in the *data* subfolder of the *labs* folder using the `readRDS()` function.

If the `readRDS` function provided below returns an error, check your working directory by typing `getwd()` into the console. The path returned is the root directory where R is looking for the file. If it's not the *labs* folder in your private repo, then change the working directory so R is looking in the right place (by using the `setwd()` function with the folder path to the *labs* folder specified, e.g., `setwd(C:/Users/kcorreia/Stat231/course-content/labs)`). You can also set your working directory by going to Session > Set Working Directory > To Source File Location. 

```{r}
#| label: get-data


# Load dataset and name it "poems" for easier reference
poems <- readRDS("../data/DickinsonPoems.Rds")
```

# Tidying text

In the first part of the lab, we'll work through pre-processing a text using the **tidytext** package.

## Tokenizing

Tokenizing a text is the process of splitting text from it's full form (e.g., paragraphs) into smaller units (e.g., sentences or lines or words). We do this with the `unnest_tokens()` function from the **tidytext** package, which takes on two main arguments: `output` and `input`. 

> `output` creates a new variable that will hold the smaller units of text, while `input` identifies the variable in your dataframe that holds the full text. In the process, we get a long version of the dataset. 

Run the code below and view the `poems_words` dataset and compare it to the `poems` dataset to see the differences. What do you notice?

> RESPONSE: It tokenizes every word, keeping it related to the poem it is from.

```{r}
#| label: tokenize-words

poems_words_all <- poems |>
  unnest_tokens(output = word, input = text)
```

The default unit for tokens is a word, but you can specify the `token =` option to tokenize the text by other functions, such as "characters", "ngrams" ($n$ words that occur together), "sentences", or "lines", among other options. How many bigrams (two-word sequences) are there? Try one or more of the alternative options, using the help as a guide, and report how many of those tokens there are. 

> RESPONSE: 89253 bigrams

```{r}
#| label: alt-tokens

poems_bigrams_all <- poems |>
  unnest_tokens(output = bigram, input = text, token = 'ngrams', n=2)

```

## Removing stop words

Many commonly used words like "the", "if", and "or" don't provide any insight into the text and are not useful for analysis. These are called *stop words* and are typically removed from a body of text before analysis. The **tidytext** package  provides a dataframe with stop words from three different lexicons ("onix", "SMART", and "snowball"). We can use this `stop_words` dataset and the `anti_join()` function to remove all the stop words from our `poems_words` dataset. 

```{r}
#| label: explore-stop-words

# Explicitly load `stop_words` into environment
data(stop_words)

# First, take a look at the `stop_words` dataset
head(stop_words)
tail(stop_words)

stop_words |>
  count(lexicon)

# goes from n=90,937 rows to n=37,063 words
# > 53,000 stop words removed!
poems_words <- poems_words_all |>
  anti_join(stop_words, by="word")
```

There are different ways we can see which stop words were removed. If you don't want all the words removed or if there are additional words that should be removed, you can modify the `stop_words` dataframe (add or remove rows) before anti-joining above. What do you think? Are there any words that have been removed that you think might be meaningful to keep?

> RESPONSE: Plenty.

```{r}
#| label: explore-removed-words

# Explore which stop words were removed
removed_words <- poems_words_all |>
  anti_join(poems_words, by = "word") |>
  count(word) |>
  arrange(word)

# another approach to viewing removed words
removed_words <- poems_words_all |>
  inner_join(stop_words, by = "word") |>
  count(word) |>
  arrange(word)
```


\newpage
# Term frequency 

Once our text has been pre-processed, we can use functions we already know and love to create a simple descriptive analysis of the term frequency.

## Common words plot

Run the code below to create a simple plot of the 10 most common words used by Emily Dickinson. 

```{r}
#| label: top-words-plot1

poems_words |>
  count(word, sort = TRUE) |>
  slice(1:10) |>
  # fct_reorder is used to re-order the axis (displaying the word) 
  # by values of n (the number of times that word was used)
  ggplot(aes(x = fct_reorder(word, n), y = n, color = word, fill = word)) +
  geom_col() +
  # Rotate graph
  coord_flip() +
  guides(color = "none", 
         fill = "none") +
  labs(
    # Remove x variable label; notice that although coordinates are flipped, 
    # the labels correspond to which variables were specified 
    # as `x` and `y` in `aes()`
    x = NULL,
    y = "Number of instances",
    title = "The most common words in Emily Dickinson's poems")
```

Run the same code but using the `poems_words_all` dataset. What do you notice about this graphic, and what does this tell us about the utility of removing stop words before analysis?

> RESPONSE: Almost all are stop words, removing them helps show which words are of real
importance to her poems.

```{r}
#| label: top-words-plot2

poems_words_all |>
  count(word, sort = TRUE) |>
  slice(1:10) |>
  # fct_reorder is used to re-order the axis (displaying the word) 
  # by values of n (the number of times that word was used)
  ggplot(aes(x = fct_reorder(word, n), y = n, color = word, fill = word)) +
  geom_col() +
  # Rotate graph
  coord_flip() +
  guides(color = "none", 
         fill = "none") +
  labs(
    # Remove x variable label; notice that although coordinates are flipped, 
    # the labels correspond to which variables were specified 
    # as `x` and `y` in `aes()`
    x = NULL,
    y = "Number of instances",
    title = "The most common words in Emily Dickinson's poems")
```

To recap, it really only took 4 commands to get from the original dataset to a dataset formatted for plotting word frequencies:

```{r}
#| label: word-freqs

poem_word_freqs <- poems |>
  # extract words as tokens
  unnest_tokens(output = word, input = text) |>
  # remove stop words in the "word" column
  anti_join(stop_words, by = "word") |>
  # count by word, adds an 'n' col and sorts descending order
  count(word, sort = TRUE) 
```

**Your turn!** Create a simple plot of the 10 most common *bigrams* used by Emily Dickinson. *Hint: you need to remove stop words AFTER creating the dataset with one row per bigram. (Why?) Create one row per bigram using the `unnest_tokens` function; then, remove any row that has at least one stop word in it.*

```{r}
#| label: top-bigrams-plot

poem_bigram_freqs <- poems |>
  unnest_tokens(output = bigram, input = text, token = 'ngrams', n = 2) |>
  separate(bigram, into = c('word1', 'word2'), sep = " ", remove = FALSE) |>
  filter(!(word1 %in% stop_words$word | word2 %in% stop_words$word)) |>
  count(bigram, sort = TRUE) |>
  drop_na() 
  
  
  


poem_bigram_freqs |>
  slice(1:10) |>
  ggplot(aes(x = fct_reorder(bigram, n), y = n, color = bigram, fill = bigram)) +
  geom_col() +
  coord_flip() +
  guides(color = "none", fill = "none") +
  labs(x = NULL, 
       y = "Instances",
       title = "Top 10 Most Frequent Bigrams Used By Emily Dickinson")
  

```


## Word clouds

Word clouds can be used as a quick visualization of the prevalence of words in a corpus.

We can get a bare-bones word cloud using the `wordcloud()` function from the **wordcloud** package. 

Note: if you get an error "Error in plot.new() : figure margins too large" or a message "[word] could not be fit on page. It will not be plotted.", try re-adjusting the size of the plotting pane and re-running the code.

```{r}
#| label: basic-word-cloud
#| fig.height: 5

# Word cloud will rearrange each time unless seed is set
set.seed(53)

# Create word cloud using tidyverse
poem_word_freqs |>
  with(wordcloud(words = word, freq = n, max.words = 50))

# Create word cloud using base R to reference variables directly
wordcloud(words = poem_word_freqs$word,
          freq = poem_word_freqs$n, 
          max.words = 50)
```

We can customize the word cloud by mapping the size and color of words to their frequency.

```{r}
#| label: custom-word-cloud

# Choose color palette from color brewer
my_palette <- brewer.pal(10, "Paired")

set.seed(53)

poem_word_freqs |>
  with(wordcloud(words = word, 
                 freq = n,
                 min.freq = 20,
                 max.words = 50,
                 # Plot the words in a random order
                 random.order = TRUE,
                 # Specify the range of the size of the words
                 scale = c(2, 0.3),
                 # Specify proportion of words with 90 degree rotation
                 rot.per = 0.15,
                 # Color words from least to most frequent
                 colors = my_palette,
                 # Change font family
                 family = "sans"))
```

**Your turn!** Create your own word cloud with 100 words and change the color scheme. Don't forget to set a seed to be able to reproduce the word cloud.

```{r}
#| label: your-word-cloud


my_palette <- brewer.pal(5, "Paired")
set.seed(100)

poem_bigram_freqs |>
  with(wordcloud(words = bigram, 
                 freq = n,
                 min.freq = 20,
                 max.words = 100,
                 # Plot the words in a random order
                 random.order = TRUE,
                 # Specify the range of the size of the words
                 scale = c(1, 0.3),
                 # Specify proportion of words with 90 degree rotation
                 rot.per = 0.15,
                 # Color words from least to most frequent
                 colors = my_palette,
                 # Change font family
                 family = "sans"))


```

\newpage
# Term frequency-inverse document frequency (tf-idf)

The idea of *tf-idf* is to find the important words in the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a corpus.

## Computing term frequency statistics 

The `bind_tf_idf()` function will compute the term frequency (tf), inverse document frequency (idf), and the tf-idf statistics for us. It requires a dataset with one row per word per poem, meaning we need a variable to indicate which poem the word comes from (`title`, in our tokenized dataset), a variable to indicate the word (`word` in our tokenized dataset), and a third variable to indicate the number of times that word appears in that specific poem. 

This time, we do not need to remove stop words. Why not?

> RESPONSE:tf-idf accounts for stop words by decreasing their importance to the 
text

```{r}
#| label: tf-stats

word_freqs_by_poem <- poems |>
  unnest_tokens(output = word, input = text) |>
  group_by(title) |>
  count(word) 

poems_tfidf <- word_freqs_by_poem |>
  bind_tf_idf(term = word, document = title, n = n)
```


## Visualizing tf-idf 

We can visualize the words with the 10 highest tf-idf values for a subset of the poems using the code below.

```{r}
#| label: tf-idf-plot
#| fig.width: 6.5
#| fig.height: 5
#| warning: FALSE

# Sample 4 poems at random
set.seed(32)

poems_subset <- sample(poems$title, size = 4)

# Compute tf-idf for each of the 4 poems
top_tfidf <- poems_tfidf |>
  filter(title %in% poems_subset) |>
  arrange(desc(tf_idf)) |>
  group_by(title) |>
  slice(1:10) 

# Plot top 10 tf-idf words
ggplot(data = top_tfidf, aes(x = fct_reorder(word, tf_idf), y = tf_idf
                             , fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~title, 
             ncol = 2, 
             scales = "free") +
  coord_flip() +
  labs(x = NULL, 
       y = "tf-idf")
```


\newpage
# Sentiment analysis

What is sentiment analysis? From [Text Mining with R](https://www.tidytextmining.com/sentiment.html) (Silge & Robinson 2019): 

*"When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust. We can use the tools of text mining to approach the emotional content of text programmatically... One way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. This isn't the only way to approach sentiment analysis, but it is an often-used approach."* 

There are different lexicons that can be used to classify the sentiment of text. Today, we'll compare two different lexicons that are both based on unigrams, the AFINN lexicon and the NRC lexicon.

## AFINN lexicon

The AFINN lexicon (Nielsen 2011) assigns words a score from -5 (negative sentiment) to +5 (positive sentiment). Check out the AFINN lexicon using the code below. What do you think of the scores? What is the rating for the word "slick"?  Does "slick" always have a positive connotation (can you think of a sentence where "slick" has a negative connotation)?

> RESPONSE: 

```{r}
#| label: afinn-lex

# Type "1" to download if prompted
afinn_lexicon <- get_sentiments("afinn")

afinn_lexicon |>
  filter(word == "slick")

afinn_lexicon |>
  count(value)
```

## NRC Word-Emotion Association lexicon

Use the `get_sentiments()` function to create a dataframe called `nrc_lexicon` that holds the NRC Word-Emotion Association lexicon (Mohammad 2010). The NRC lexicon catergoizes words as yes/no for the following sentiment categories: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. What does each row in this dataset represent? (*Hint*: it's *not* the same as the `afinn_lexicon` dataset.)

> RESPONSE: 

```{r}
#| label: nrc-lex

nrc_lexicon <- get_sentiments('nrc')

nrc_lexicon |>
  filter(word == 'hug')

```

## User (and Consumer!) Beware

Do you see any issues in applying these lexicons (developed fairly recently) to the Emily Dickinson poems?

> RESPONSE: Word sentiment changes over time
 
The provided lexicons are based on unigrams. Do you see any disadvantages of basing the sentiment on single words?

> RESPONSE: 

We can calculate how many words used in the poems are not found in the lexicons using the code below. List a few words that are not in the NRC lexicon that appear in the poems. What proportion of unigrams observed within this corpora of Dickinson poems are *not* scored by the NRC lexicon?

> RESPONSE: Over half.

```{r}
#| label: missed-words

# identify words in word_frequencies dataset (which has stop words removed) that are not the NRC lexicon
nrc_missed_words <- poem_word_freqs |>
  anti_join(nrc_lexicon, by = "word")
```

With these (rather important!) drawbacks in mind, let's go ahead and view the top words by sentiment classified by the NRC lexicon. That is, create a figure of the top 10 words under each sentiment, facetted by sentiment, for the following sentiments: anger, anticipation, fear, joy, surprise, and trust. You can use code given in earlier chunks to guide you.

```{r}
#| label: nrc-sentiment
#| fig.height: 6

poem_word_freqs |>
  filter(word %in% nrc_lexicon$word) |>
  left_join(nrc_lexicon, by="word") |>
  group_by(sentiment) |>
  ggplot(aes(word, y=n)) +
  facet_wrap(~sentiment)
```

How might you summarize the sentiment of this corpus using the AFINN lexicon?

> RESPONSE:  

```{r}
#| label: afinn-sentiment


```


# References 

## AFINN Lexicon

Nielsen, FA. A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings 93-98. 2011 May. http://arxiv.org/abs/1103.2903.

## NRC Lexicon 

Mohammad S, Turney P. Crowdsourcing a Word-Emotion Association Lexicon. *Computational Intelligence*. 2013;29(3):436-465.    

Mohammad S, Turney P. Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon. In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California. http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

## Text Mining with R 

Silge J, Robinson D (2016). "tidytext: Text Mining and Analysis Using Tidy Data Principles in R." *JOSS*, *1*(3). doi: [10.21105/joss.00037](https://doi.org/10.21105/joss.00037).

Silge J, Robinson D (2017). Text Mining with R: A Tidy Approach. O'Reilly Media Inc. Sebastopol, CA. https://www.tidytextmining.com/index.html
