---
title: 'STAT 231: Practice Set 4'
subtitle: "due by 9 PM on Sunday, 3/9"
author: "Fynn Hayton-Ruffner"
format: pdf
---

# Instructions 

Practice set assignments are designed to help you further ingest and practice the material covered in class over the past week(s).

You may work through these problems with peers, but all work must be completed by you (see the Honor Code in the syllabus) and you must indicate who you discussed the assignment with (if anyone).  

Even then, the best approach here is to try the problems on your own before discussing them with peers, and then write your final solutions yourself.

## GitHub workflow  

1. Before editing this file, verify you are working on the copy saved in YOUR repo for the course
2. Before editing this file, make an initial commit of the file to your repo to add your copy of the problem set.
3. Change your name at the top of the file and get started!
4. You should save and commit the .qmd file each time youâ€™ve finished a question, if not more often.
5. You should occasionally push the updated version of the .qmd file back onto GitHub. 
6. When you are done with the assignment, knit to pdf, and commit and push both the .qmd and PDF to GitHub 

## Gradescope upload

For each question, allocate all pages associated with the specific question. If your work for a question runs onto a page that you did not select, you may not get credit for the work. If you do not allocate any pages when you upload your PDF, you may get a zero for the assignment.

# Academic Integrity 

If you worked with others or used resources outside of provided course material (notes, textbook, etc) to complete this assignment, please acknowledge them below using a bulleted list. 

*If you discussed this assignment with any of your peers, please list who here:*

Name(s) and corresponding problem(s)

*. N/A

*If you used resources outside the provided course material (notes, textbook, etc.) including generative AI, please list here:*

Source(s) (including prompt if a generative AI source) and corresponding problem(s)

* N/A


```{r}
#| label: set-up
#| include: false

knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small",   # slightly smaller font for code
  message=FALSE)   

# load packages here
library(tidyverse)
library(tidytext)
library(robotstxt)
library(rvest)
library(wordcloud)
library(kableExtra)
library(stringr)
library(textdata)


```


\newpage
# Scraping Tables

For this exercise, your task is to:

1. Scrape a table of your choosing from a page we have not yet scraped (yes, it can be from a different Wikipedia page; no, it cannot be a table you are scraping for your Shiny project).  
2. Clean it up, as needed, in preparation for a professional quality display.
3. Display *a few (no more than 10)* rows of the scraped table using `kable()`, customizing the variable names and other aspects of the table's display so that it is publication-ready.

Note: You must be sure that scraping the table is allowed. Your code should show appropriate documentation of your steps.

> DESC: I'm scraping a static table from Basketball-Reference.com of the per-game
stats of warriors players. I'm going to display the basic stats of the top 10
highest scorers on the team. 

```{r}
#| label: scrape-table
#| eval: false

# check permissions ------------------------------------------------------------
warriors_stats_url <- 
  'https://www.basketball-reference.com/teams/GSW/2025.html#all_per_minute_stats'
paths_allowed(warriors_stats_url)

# scrape the table -------------------------------------------------------------

warriors_table <- warriors_stats_url |>
  read_html() |>
  html_element('#per_game_stats') |>
  html_table()

# clean up the table -----------------------------------------------------------

w_pergame_stats <- warriors_table |>
  # selecting variables of interest
  select(Player, G, MP, FG, FGA, TRB, AST, PTS) |>
  # only keep players who have played 10 or more games
  filter(G >= 10) |>
  # sort by points
  arrange(desc(PTS)) |>
  # the first obs is a team total row so I drop that
  slice(2:length(warriors_table))
  
  

# SAVE as permanent file at end of this code chunk -----------------------------
save(w_pergame_stats, file = 'w_pergame_stats.RData')

```

```{r}
#| label: pubqual-table

# import/load saved file at beginning of this code chunk -----------------------

load('w_pergame_stats.RData')


# use kable to print publication-ready table -----------------------------------

w_pergame_stats |>
  slice(1:10) |>
  kable(booktabs = TRUE,
        col.names = c('Player', 'Games', 'Mins', 'FG', 'FG Attempts', 'Rebs', 
                      'Assists', 'Points'), 
        caption = "Top 10 Warriors in PTS/Game Essential Stats") |>
    kable_styling(latex_options = "striped")

```


*Be sure to commit and push your changes to GitHub.*

\newpage
# MDSR 19.5 (modified)

(a) Wikipedia defines a hashtag as "a type of metadata tag used on social networks such as Twitter and other microblogging services, allowing users to apply dynamic, user-generated tagging which makes it possible for others to easily find messages with a specific theme or content. A hashtag must begin with a hash character followed by other characters, and is terminated by a space or end of message. It is always safe to precede the # with a space, and to include letters without diacritics (e.g., accents), digits, and underscores." 

Use one of the **stringr** package functions to create the following vectors (which should be the same length as the `my_example_string` vector):

- a vector `hashtag_any` which identifies whether or not there is a hashtag in each element  of the `my_example_string` vector (i.e., returns TRUE/FALSE)  
- a vector `hashtag_num` which identifies the number of times a hashtag appears in each element of the `my_example_string` vector  
- a vector `no_hashtags` which removes all the hashtags from each element of the `my_example_string` vector

Print the vectors so their contents are displayed in the output.

```{r}
#| label: hashtags-a

my_example_string <- c(
  "This string has no hashtags",
  "#hashtag city!",
  "This string has a #hashtag",
  "This string has #two #hashtags",
  "This string has a #hashtag! with an exclamation point in it",
  "This string has a #1hashtag that starts with a number" 
)

# detect #'s
hashtag_any <- my_example_string |>
  str_detect('#')

# count #'s 
hashtag_num <- my_example_string |>
  str_count('#')

# remove all #'s
no_hashtag <- my_example_string |>
  str_remove_all('#')

# printing to console
print(hashtag_any)
print(hashtag_num)
print(no_hashtag)
```

(b) Run the code below and identify what each call of the `str_extract_all()` function is doing. In particular, what is different about the output? What might this imply for the difference between the regular expressions used ("\\S+" versus "\\w+")?

*Hint: check out the regular expressions section of the stringr cheatsheet (on Moodle)*

> RESPONSE: The only difference is that for the 5th string in the vector, the '!' 
is left out by the second call (#hashtag! vs #hashtag). Since both match the '1' in the 6th string, this indicates that \\S matches any non whitespace character (based off the fact that \\s is whitespace), while \\w seems to match only alphabetical 
characters or numbers. I think it is called a 'word' character.

```{r}
#| label: hashtags-b

# first call of function
str_extract_all(my_example_string, pattern="#\\S+")


# second call of function
str_extract_all(my_example_string, pattern="#\\w+")
```


\newpage
# Netflix 

We have a dataset on a collection of >5,000 movies and television shows on Netflix with information about the title, director, cast, release year, date added to Netflix, rating, duration, and description.

(a) Warm-up! Create a histogram of the duration of Documentaries and Comedies, colored by the genre (Documentary or Comedy). You will need to do some (minimal) data wrangling to create a duration variable and a genre variable that's useable for plotting. Base the Documentary/Comedy classification on the `listed_in` variable. Be sure to label your axes appropriately!

In 1-2 sentences, compare the duration of documentaries and comedies.

> RESPONSE: For one there are a lot more comedies than documentaries, but they
tend to center around the same-ish duration of 100 minutes. That said, there 
are clearly more comedies that run longer than that, so in general, comedies tend to 
be a little longer than documentaries.

```{r}
#| label: netflix-a

netflix <- readRDS("../data/netflix_titles.Rds")

wrangled <- netflix |>
  # add genre variable, can use extract here to get Documentaries/comedies 
  # from listed in
  mutate(genre = str_extract(listed_in, pattern= 'Documentaries|Comedies')) |>
  # filter to retain only genres that match the pattern below
  # could also do drop na at this point because genre is NA if its listed_in
  # variable did not include either movie genre in the call above
  filter(str_detect(genre, pattern= 'Documentaries|Comedies'))

# extract only the digits in the duration col, convert to number type
wrangled$duration <- as.numeric(str_extract(wrangled$duration, '\\d+'))

# plot histogram
wrangled |>
  ggplot(aes(x=duration, fill = genre)) +
  geom_histogram() +
  labs(
    x="Duration (minutes)",
    y="Frequency",
    title = "Distribution of Duration for Comedies and Documentaries",
    caption = "Data obtained from Netflix",
    fill = ""
  ) +
  theme(legend.position = "top",
        title = element_text(size=11)) 
  

```

(b) Classify "The Angry Birds Movie 2" movie as either positive or negative using the AFINN lexicon. Does this match with your own interpretation of the movie description (or your own interpretation of the movie, if you've seen it)? How many words are in the movie description? How many of the words in the movie description are defined in the AFINN lexicon? How might this impact the sentiment analysis (e.g., if we extended it to all children's movies)? 

> RESPONSE: The score (sum of unigram sentiment scores) is -4. This classifies the 
description/movie as negative. I would argue it doesn't capture any nuance. Yes there
are some 'negative' words, but my interpretation for the description is one of 
hope and unitedness in the face of adversity, so I would call it more positive. 
There are 27 tokens, (25 words) in the description, but only two are included in the Afinn lexicon. This makes the classification super inaccurate, as the meanings of all those missed words, (most of which are important) are reduced to 0, as the model relies entirely on 'threatening' and 'enemies' to classify the description. This applies to the rest of the movies as well: no classification will be very accurate.

```{r}
#| label: netflix-b

# load in afinn
afinn_lexicon <- get_sentiments("afinn")

# get just the angry birds movie
angry_birds_2 <- netflix |>
  filter(title == "The Angry Birds Movie 2")

# print description
print(angry_birds_2$description)

description_tokens <- angry_birds_2 |>
  # get the tokens from the description
  unnest_tokens(output = word, input = description) |>
  # keep only the word
  select(word)

description_sentiments <- description_tokens |>
  # get the sentiments for each word
  left_join(afinn_lexicon, by="word") |>
  # group by word, not necessary but I can sum each words sentiment this way
  group_by(word) |>
  summarize(
    num_words = n(),
    sentiment = sum(value, na.rm = TRUE),
    .groups = "drop"
    
  )

# get the cumulative sentiment score, this is based on the description of 
# sentiment score as defined by lab 8: 
# One way to analyze the sentiment of a text is to consider the text as a 
# combination of its individual words and the sentiment content of the whole 
# text as the sum of the sentiment content of the individual words. 

# I'm just summing up the sentiment scores to get a cumulative score, not an 
# average

sentiment_score <- sum(description_sentiments$sentiment)

# overall score is -4, since most are 0.
print(sentiment_score)


```

(c) Visualize the words with the 5 highest tf-idf values for the four randomly sampled movies below *OR* for four movies of your choice. Note that you should compute the tf-idf of each word from the movie descriptions based on the full corpus of *all* movie descriptions (not just the four you're going to visualize).

*Hint: see solutions to Lab 8.*

```{r}
#| label: netflix-c

# Sample 4 movies at random
set.seed(231)
movies_sample <- sample(netflix$title, size = 4)

# titles are 10,000 BC, Lampoon..., Big Time Movie, & Motor Matraan Di
print(movies_sample)


word_freqs_by_desc <- netflix |>
  # get tokens from each movie description
  unnest_tokens(output = word, input = description) |>
  group_by(title) |>
  # count words based on title
  count(word)

# with word frequency by title, we can call bind_tf_idf on the dataset created
# above
movies_tfidf <- word_freqs_by_desc |>
  bind_tf_idf(term=word, document = title, n = n)


# now create dataset for top 5 tf_idf words per movie
selected_movies_scores <- movies_tfidf |>
  # retain only movies from random movies_sample 
  filter(title %in% movies_sample) |>
  # arrange each title in descending order of tf_idf so we can slice the top 5
  arrange(title, desc(tf_idf)) |>
  # group by title 
  group_by(title) |>
  # the call to slice here slices by group, not by the entire dateset, so we are
  # still left with 5obs / movie title (the top 5 tfidf words)
  slice(1:5)

# plot each faceted by title
selected_movies_scores |>
  ggplot(aes(x = fct_reorder(word, tf_idf), y = tf_idf, fill = as.factor(tf_idf))) +
  geom_col() +
  coord_flip() +
  theme(legend.position = "none") +
  facet_wrap(~title, ncol=2, scales ="free") +
  labs(x = NULL, 
       y = "TF-IDF",
       title = "Top 5 words by Tf-Idf for 4 Netflix Films")




```


